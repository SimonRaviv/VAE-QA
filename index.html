<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="We leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time.">
  <meta property="og:title" content="Assessing Image Quality Using a Simple Generative Representation" />
  <meta property="og:description"
    content="We leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time." />
  <meta property="og:url" content="https://simonraviv.github.io/VAE-QA/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Assessing Image Quality Using a Simple Generative Representation">
  <meta name="twitter:description"
    content="We leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="IQA, Image Quality Assessment, Generative Models, VAE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Assessing Image Quality Using a Simple Generative Representation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Assessing Image Quality Using a Simple Generative Representation
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/simon-raviv-28976369" target="_blank">Simon
                  Raviv</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal
                  Chechik</a><sup>1,2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Bar-Ilan University,</span>
              <span class="author-block"><sup>2</sup>NVIDIA Research</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2404.18178" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <!-- <a href="https://github.com/SimonRaviv/VAE-QA/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark"> -->
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (TBD)</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          We leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image
          quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find
          that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller
          memory footprint and faster run time.
        </h2>
        <img src="static/images/vaeqa_arch_v2.jpg" alt="VAE-QA architecture" class="center-image">
        <h2 class="subtitle has-text-centered">
          <p>
            Our VAE-QA architecture uses Variational Autoencoders (VAEs) to generate image representations. These are
            then combined to form a compressed representation. This combined representation is used to predict the
            quality score of input images by a MLP network.
          </p>
          <div class="container is-max-desktop">
            <hr>
            <h2 class="subtitle hero is-4">Main contributions</h2>
            <p>
              <b>(1)</b> We propose that predicting image quality would be superior
              using a representation learned in a generative way. <b>(2)</b> A new architecture for predicting image
              quality built on top of a VAE model, which learns to align features from several different layers of the
              VAE. <b>(3)</b> Standardization of the evaluation protocol. <b>(4)</b> New SoTA results for cross-dataset
              generalization.
            </p>
          </div>
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Perceptual image quality assessment (IQA) is the task of predicting the visual quality of an image as
              perceived by a human observer. Current state-of-the-art techniques are based on deep representations
              trained in discriminative manner. Such representations may ignore visually important features, if they are
              not predictive of class labels. Recent generative models successfully learn low-dimensional
              representations using auto-encoding and have been argued to preserve better visual features. Here we
              leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image
              quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find
              that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller
              memory footprint and faster run time.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Main results</h2>
      </div>
      <!-- Cross-dataset evaluation -->
      <div class="container is-max-desktop">
        <hr>
        <h2 class="title is-4">Cross-dataset evaluation</h2>
        <hr>
        <div class="level-set has-text-justified">
          <p>
            To evaluate the generalization performance of the model, we trained the model on the entire KADID-10k
            dataset and evaluated it on the full set of LIVE, CSIQ, and TID2013 datasets. The table below report the
            average PLCC and SRCC values of this experiment. VAE-QA consistently improves over current methods.
          </p>
          <img src="static/images/cross_dataset_kadid10_results.jpg" alt="Cross-dataset KADID-10k results"
            class="blend-img-background center-image" />
          <p>
            We further tested generalization by training the model on the TID2013 dataset and testing on the LIVE and
            CSIQ datasets. The table below shows the results of this experiment. Our model generalizes robustly to new
            data distributions.
          </p>
          <div class="has-text-centered">
            <img src="static/images/cross_dataset_tid2013_results.jpg" alt="Cross-dataset TID2013 results"
              class="blend-img-background center-image" style="width: 80%; height: auto;" />
          </div>
        </div>
      </div>
      <!-- End Cross-dataset evaluation -->

      <!-- Within-dataset evaluation -->
      <div class="container is-max-desktop">
        <hr>
        <h2 class="title is-4">Within-dataset evaluation</h2>
        <hr>
        <div class="level-set has-text-justified">
          <p>
            We also evaluate the model by training and testing it on images from the same data distribution.
            The table below shows the linear and non-linear correlation over the test split for each of the 3 standard
            datasets. For LIVE and CSIQ datasets, VAE-QA shows a small improvement, while it improves considerably for
            TID2013 and across datasets overall.
          </p>
          <div class="has-text-centered">
            <img src="static/images/within_dataset_results.jpg" alt="Within-dataset results"
              class="blend-img-background center-image" style="width: 90%; height: auto;" />
          </div>
          <p>
            The figure below shows the relation between predicted and ground-truth quality. The scatter plot suggests
            this relation is largely linear in this regime.
          </p>
          <img src=" static/images/within_eval_mos_vs_predicted_mos.jpg" alt="Scatter plot"
            class="blend-img-background center-image" />
        </div>
      </div>
      <!-- End Within-dataset evaluation -->

      <!-- Correlation by distortion type -->
      <div class="container is-max-desktop">
        <hr>
        <h2 class="title is-4">Correlation by distortion type</h2>
        <hr>
        <div class="level-set has-text-justified">
          <p>
            To obtain more insight into the performance of VAE-QA, we measured the quality of quality prediction for
            different types of distortion. The figure below compares the prediction accuracy measured using PLCC, for
            our method and AHIQ, the current SoTA. The VAE representation underlying VAE-QA, appears to help in all
            distortion types, and provides on average larger improvements for those types that are challenging for
            AHIQ
            (masked noise and intensity shift).
          </p>
          <img src="static/images/distortion_type_results.jpg" alt="Distortion type results"
            class="blend-img-background center-image" />
        </div>
      </div>
      <!-- End Correlation by distortion type -->
  </section>
  <!-- End Results-->

  <!-- Runtime and Memory Footprint -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <hr>
        <h2 class="title is-3 has-text-centered">Runtime and Memory Footprint</h2>
        <hr>
        <p>
          We compare the memory footprint and runtime of our method with the current state-of-the-art
          method AHIQ. Our method demonstrates a smaller memory footprint and faster inference time.
        </p>
        <div class="has-text-centered">
          <img src="static/images/model_size.jpg" alt="Memory footprint" class="blend-img-background center-image"
            style="width: 80%; height: auto;" />
          <br>
          <br>
          <img src="static/images/inference_time.jpg" alt="Inference time" class="blend-img-background center-image"
            style="width: 30%; height: auto;" />
        </div>
      </div>
    </div>
  </section>
  <!-- End Runtime and Memory Footprint -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <p>If you find our work useful, please cite our paper:</p>
      <pre><code>@misc{raviv2024assessing,
      title={Assessing Image Quality Using a Simple Generative Representation},
      author={Simon Raviv and Gal Chechik},
      year={2024},
      eprint={2404.18178},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <!-- End Footer -->

  <!-- Statcounter tracking code -->
  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
  <!-- Default Statcounter code for VAE-QA https://simonraviv.github.io/VAE-QA/ -->
  <script type="text/javascript">
    var sc_project = 12992786;
    var sc_invisible = 1;
    var sc_security = "6995c9ca"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12992786/0/6995c9ca/1/" alt="Web Analytics"
          referrerPolicy="no-referrer-when-downgrade"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>